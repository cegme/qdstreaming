
\section{Algorithms}

In this section we will describe XXX algorithms for entity sampling.
The main method we use to improve the computation speed is to reduce the
number of comparisons we make for large entity clusters.
We additionally discuss methods to improve the performance of algorithms
over time by collecting statistics from the processes.

\textbf{Naive Iterator.} This method performs pairwise comparisons by 
iterating over the mentions using the order on disk. This is the traditional
method of computing the pairwise similarity of two clusters. 

\textbf{SubSample Iterator.} This method performs uniform samples of the
mentions from the source and destination entities clusters. This method
measures the confidence of the calculated pairwise samples and stops when the
confidence exceeds a threshold of 0.95.

\textbf{Top-K SubSample Iterator.} This method uses a priority queue to sort
the mentions and only performs comparisons between the top $M$ mentions.

\textbf{Blocked Iterator.} This method performs blocking on the mentions in the
source and destination entities and takes the average gain from pairwise
comparisons inside each block. Blocks are formed arbitrarily and are of fixed
size. In database terms this method is a \emph{block nested loop} comparison.

\textbf{Blocked Subsample Iterator.} This performs a block nested loop
comparison but it performs book keeping to only perform pairwise sampling
until we reach a confidence threshold of 0.95.

\textbf{Blocked Top-k Iterator.} This method performs a block nested loop
comparison except blocks are created by reading mentions from a priority queue.

\textbf{Blocked Top-K Subsample Iterator.} This method is a combination of the
Block subsample iterator and the blocked top-k iterator.

Further, We examine active learning techniques to adjust threshold sizes based
on statistics collected while running the algorithms.

We collect the following statistics from each training run:

Success and failure
Data set size.
Number of uniques tokens.
Average pairwise score between mentions.
Maximum pairwise score between mentions.
Minimum pairwise score between mentions.
Variance of the pairwise score between mentions.
Mention Token tf-idf scores.
Cardinality of tokens in both entities.
\ceg{Generalize the feature explanation and move it to the implementation section}

Using this information we train a decision tree classifier to minimize the 
pairwise comparisons in the score function.
The classifier is also constrained by the accuracy of the decision tree as 
approximate methods are less accurate.
More formally \ldots

The result of this classifier is two-fold.
First, we have a classifier to choose the optimal algorithm for each proposal.
Second, we have an active learning feed-back loop for algorithms with 
thresholds.
We empirically study both of these outcomes.

\begin{tabular}{l c c c}
\textbf{Technique} & \textbf{Reduces size} & \textbf{Lossless} & \textbf{location} \\
Full pairwise & No & Yes & Feature\\
Early Stopping~\cite{singh2012monte} & No & No & Feature\\
Run-Length Encoding & Yes & Yes & Storage\\
\end{tabular}



\section{Optimizer}

When before calculating the MCMC-MH proposal there are several decision we can make
that will affect the runtime and accuracy of the algorithm.
At each step we may:
  (1) Update an entity structure to a compressed format;
  (2) Select a new way of calculating the pairwise features;
  (3) Skip the calculation of the proposal and directly accept or reject.
These decisions can be made by observing several features of a source entity,
destination entity and a source mention.
%These features include the source and destination size, the source and
%destination cardinality, the number of samples the algorithm has made.
We enumerate a small set of features that can yield information to
help us decide how the entity structure should be changed.

\paragraph{Compressing an entity} The decision to compress an entity takes
four main points into consideration. First, the time it takes to compress
the entity ($ C_\text{time}$).
For example, if the time it takes to compress an entity is the same as the time it takes to
reach an answer in the uncompressed format, then compression is superfluous.
Secondly, it is important to consider the spaced saved in memory and the amount of 
additional entities that do not have to be fetched from disk and can now fit in memory ($C_\text{space}$).
Third, we need to know how active an entity has been ($C_\text{active}$).
That is, how many additions or subtractions this entity has seen over a long period of time.
This information is helpful in understanding the likelihood this entity will be requested
for another addition or subtractions.
Last, we retain the activity of an entity over a recent, short period of time ($C_\text{velocity}$). This information lets us know whether it is smart for this entity to take the time out to
for compression while other mentions may be attempting an insertion or removal.

  At each proposal step the decision made should maximize the \textit{utility}.
  Utility of the decision is a numeric score to represent the gain performing
  the proposal calculation. The utility value is a real number ranged from $( \inf, \int)$.


  A formal model for utility is as follows:
  \begin{equation}
    \begin{array}{ll}
  U = & C_\text{time} +\\ 
      & C_\text{space} +\\
      & C_\text{active} +\\ % The cost of updating the entity structure to a compressed one., (Good value if it is already compressed)
      & C_\text{velocity} % Undoing the mention change (the certainty/assurity that this is correct)
      \end{array}
  \end{equation}





