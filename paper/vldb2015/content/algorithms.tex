
\section{Algorithms}

In this section we will describe XXX algorithms for entity sampling.
The main method we use to improve the computation speed is to reduce the
number of comparisons we make for large entity clusters.
We additionally discuss methods to improve the performance of algorithms
over time by collecting statistics from the processes.

\textbf{Naive Iterator.} This method performs pairwise comparisons by 
iterating over the mentions using the order on disk. This is the traditional
method of computing the pairwise similarity of two clusters. 

\textbf{SubSample Iterator.} This method performs uniform samples of the
mentions from the source and destination entities clusters. This method
measures the confidence of the calculated pairwise samples and stops when the
confidence exceeds a threshold of 0.95.

\textbf{Top-K SubSample Iterator.} This method uses a priority queue to sort
the mentions and only performs comparisons between the top $M$ mentions.

\textbf{Blocked Iterator.} This method performs blocking on the mentions in the
source and destination entities and takes the average gain from pairwise
comparisons inside each block. Blocks are formed arbitrarily and are of fixed
size. In database terms this method is a \emph{block nested loop} comparison.

\textbf{Blocked Subsample Iterator.} This performs a block nested loop
comparisom but it performes book keeping to only perform pairwise sampling
until we reach a confidence threshold of 0.95.

\textbf{Blocked Top-k Iterator.} This method performs a block nested loop
comparison except blocks are created by reading mentions from a priority queue.

\textbf{Blocked Top-K Subsample Iterator.} This method is a combination of the
Block subsample iterator and the blocked top-k iterator.

Further, We examine active learning techniques to adjust threshold sizes based
on statistics collected while running the algorithms.

We collect the following statistics from each training run:

Success and failure
Data set size.
Number of uniques tokens.
Average pairwise score between mentions.
Maximum pairwise score between mentions.
Minimum pairwise score between mentions.
Variance of the pairwise score between mentions.
Mention Token tfidf scores.
Cardinality of tokens in both entities.
\ceg{Generailze the feature explaination and move it to the implementation section}

Using this information we train a decision tree classsifier to minimize the 
pairwise comparisons in the score function.
The classifier is also constrained by the accuracy of the decision tree as 
approximate methods are less accurate.
More formally \ldots

The result of this classifier is two-fold.
First, we have a classifier to choose the optimal algorithm for each proposal.
Second, we have an active learning feed-back loop for algorithms with 
thresholds.
We emperically study both of these outcomes.

\begin{tabular}{l c c c}
\textbf{Technique} & \textbf{Reduces size} & \textbf{Lossless} & \textbf{location} \\
Full pairwise & No & Yes & Feature\\
Early Stopping~\cite{singh2012monte} & No & No & Feature\\
Run-Length Encoding & Yes & Yes & Storage\\
\end{tabular}





