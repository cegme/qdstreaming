
\section{Example}

In this section we discuss define the acceleration in MCMC-MH sampling for entity resolution.
We then motivate how we believe gains can be achieved given using compression, sampling acceleration methods and optimizers.

The issues we are investigating are as follows.
First, Given an Entity $e_i$ how can we create a structure with \textit{minimal total size}.
%Second, How can we incrementally add and remove items 
%from the compressed structure.
Next, how can we compute the features over a source and destination entity ($e_s$, $e_d$) 
in the \textit{smallest amount of time}.
Lastly, how do we \textit{decide} when to use each technique.


The total size of all entities in the traditional representation is:

\begin{equation}
  \text{sizeof}(\mathcal{E}) =  \sum_i c + (\text{sizeof}(\text{int}) * |e_i|),
\end{equation}
where $sizeof$ is an abstract function to compute the size of the containing object,
$c$ is a class constant and $|e_i|$ is number of mentions in the entity.

There are many compression techniques, one being we only keep the mentions an entity
that have a unique representation.
That is, if any mention token is a duplicate, we remove it.
This compressed total entity size size is:

\begin{equation}
  \text{sizeof}(\mathcal{E_\text{compressed}}) = \sum_i c + (\text{sizeof}(int) * \#(e_i) ),
\end{equation}
where $\#(e_i)$ is the cardinality of the mention tokens in entity $e_i$.
We note that when the $\#(e_i) \ll |e_i|$ it may be worth compressing this entity.


In the example data set in Figure~\ref{fig:entity-distribution} we note that
45\% percent of mentions are smaller that 100 mentions in size.
Additionally, 82\% percent of entities contain less than 1000 mentions.
These number suggest that at times we 
we can take advantage of the redundancy within
large entities and compress the entities.
(We investigate the WikiLinks corpus further in Section~\ref{sec:microbenchmark}). 



In addition, Figure~\ref{fig:entity-distribution} show that there is about an
order of magnitude difference between the sizes of initial entities and the
true entity sizes. This suggest that there are several unique representations
of entities that during entity resolution entities sizes can expect to increase
by an order of magnitude in size and smaller entities will decrease.
This is to be expected but we can use this property to track the grow and
change of entity sizes over time to understand which camp a particular entity
cluster belongs. Either it will grow or it will probably shrink.

