
\section{Background}
\label{sec:optimizer:background}

Factor graphs
are a pairwise formalism for expressing arbitrarily complex
relationships between random variables~\cite{koller2009probabilistic}.
A factor graph $\mathcal{F} = \langle \textbf{x}, \psi \rangle$, contains a set of random
variables $ \textbf{x} = {\{x_i \}}^n_1$ and
factors ${\textbf{\boldmath$\psi$\unboldmath}} = { \{\psi_i\} }^m_1 $.
Random variables are connected to each other through factors.
Factors are a mapping between one or more variables and a real-valued score.

The probability of a setting $\mathbf{\omega}$ among the set of all possible
settings $\Omega$ occurring in a factor graph is given by a probability measure:
\begin{align*}
\pi(\mathbf{\omega}) = \frac{1}{Z} \sum_{x \in \omega} \prod_{i=1}^{m} \psi_i(x^i) &,&  Z = \sum_{\omega \in \Omega} \sum_{x \in \omega}  \prod_{i=1}^{m} \psi_i(x^i) 
\end{align*}
where $x^i$ is the set of random variables that neighbor the factor
$\psi_{i}(\cdot)$ and $Z$ is the normalizing constant.


%Markov Chain Monte Carlo Metropolis Hastings is a techniques for performing
Exact inference over complex factors graphs is computationally expensive
because it involves computing the normalizing constant.
Therefore, it is popular for researchers to use Markov Chain Monte Carlo (MCMC) 
approximation techniques to estimate the probability of settings.
In particular, for large and dense factor graphs MCMC Metropolis Hastings (MH) has
been shown to be a scalable technique for inference calculation~\cite{singh2011large}.

Cross-Document entity resolution, resolving entities across document borders,
is usually several orders of magnitude smaller when compared to within document entity resolution.
In large text corpora, the size of entities follows the power law~\cite{singh12:wiki-links}.
For example, Figure~\ref{fig:entity-distribution} is a generated data set containing
40 million mentions and 3 million entities over 11 million web pages.
As documents and mentions are incrementally streamed through,
the scale problem becomes a critical issue.

The mentions on disk can be represented as a large array of identifiers.
Entities are a collection of mentions and can be represented as such.
In the worst case there is an equal number of entities and mentions.
This means each mention is its own individual entity.
In the other extreme, all the mentions may be a part of the same entity.
For streaming entity resolution, mentions within documents
must be matched to the existing set of entities~\cite{rao2010streaming}.
In this paper, we assume the entity set is initialized by grouping the most
similar mentions; new mentions are assigned to the closed match.

To compute the score at each step, the number of comparisons is proportional to the number of pairwise factors between mentions.
The pairwise factors are weighted functions such as approximate string matches, token overlap, n-gram matches.
There are additional cluster-wide features calculated at each step.
Such features include functions to check whether all mentions in a cluster share the same token.
For clusters larger than 1000 mentions, calculating scores of the model becomes extremely expensive.
Performing sophisticated techniques over smaller clusters also adds extra overhead.
In this paper, we examine the trade-off of selecting methods to
accelerate the feature computation process.


\begin{figure}
\centering
\includegraphics[width=\columnwidth]{media/start-vs-nd.png}
\caption{A distribution of entity sizes from the wiki links corpus~\cite{singh12:wiki-links} with an initial start and the truth.}
\label{fig:entity-distribution}
\end{figure}


\eat{%
\section{Related Work}
\label{sec:optimizer:relatedwork}

In this paper, we focus on the creation of optimizers for sampling-based ER over factor graphs.
We observe two primary methods for increasing sampling speed.
The first is a method for approximating the score of factor counts, and second is a compression method.
We investigate three papers with that state-of-the-art techniques, respectively, for
 (1) early stopping methods,
 (2) entity resolution compression,
 (3) streaming entity resolution.
We use these works as a backdrop for the development of the optimizer.

Singh et al.\ describe a method of approximating the factor computation during MCMC
inference over factor graphs~\cite{singh2011large}.
In this work, the authors note that obtaining a sample is usually inexpensive
except in the case of many variables. To this end, they introduce Monte Carlo
MCMC; this is a method of performing uniform and confidence-based sampling
over the computation of factors to avoid the full computation of features.
This work shows significant speedup in large-scale experiments.

% Slides: http://people.cs.umass.edu/~sameer/files/largescale-acl11-ppt.pdf
% Slides: http://people.cs.umass.edu/~sameer/files/mcmcmc-emnlp12-ppt.pdf

Wick et al.\ perform hierarchal compression of entities during
entity resolution~\cite{wick2013discriminative}.
This work defines a recursive data structure that allows succinct representations
of millions of mentions. 
The hierarchical structure adds some book keeping to the sampling process but because
large entity clusters are compressed there are less comparison for each sampling iteration. 
This method obtains orders of magnitude speedup
when compared to a pairwise method.

Rao et al.\ create a system for streaming entity resolution~\cite{rao2010streaming}.
They use the doubling algorithm~\cite{charikar1997incremental} to grow sets of clusters
and a clever disk arrangement strategy~\cite{omiecinski1984global} to retrieve
growing entities on demand.
However, this paper outlines no long-term strategy for managing growing entities.
The same pain points we identified with streaming entity resolution exist in this work.

%As we see, in Section~\ref{sec:microbenchmark} the entity sizes have a
%large effect on the factor computation size.
}

