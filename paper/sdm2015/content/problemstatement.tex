
\section{Accelerating Entity Resolution}
\label{sec:optimizer:example}

In this section, we discuss the acceleration in MCMC-MH sampling for entity resolution.
We then motivate how we believe gains can be achieved given using compression,
sampling acceleration methods and optimizers.
We use a large real-world corpus to a motivating example.

The two issues we are investigating are as follows:
%First, given an entity \(e_i\) how can we create a structure with \textit{minimal total size}.
%Second, How can we incrementally add and remove items 
%from the compressed structure.
First, given a source and 
destination entity \((e_s, e_d)\) and the mention, how can we score the proposal in the \textit{least amount of time}.
Secondly, after the proposal is calculated, should we compress the entity structure?
The optimizer will \textit{decide} when to use each technique.

The total size of all entities in the traditional representation is:
\begin{equation}
  \text{sizeof}(\mathcal{E}) =  \sum_i c + (\text{sizeof}(\text{int}) * |e_i|),
\end{equation}
where $sizeof$ is an abstract function to compute the size of the containing object,
$c$ is a class constant and $|e_i|$ is number of mentions in the entity.

There are many compression techniques, one being we only keep the mentions an entity
that have a unique representation.
That is, if any mention token is a duplicate, we remove it.
This compressed total entity size size is:

\begin{equation}
  \text{sizeof}(\mathcal{E_\text{compressed}}) = \sum_i c + (\text{sizeof}(int) * \#(e_i) ),
\end{equation}
where $\#(e_i)$ is the cardinality of the mention tokens in entity $e_i$.
We note that when the $\#(e_i) \ll |e_i|$, it may be worth compressing the entity $e_i$.


In Figure~\ref{fig:entity-distribution}, we note that
45\% percent of mentions are smaller that 100 mentions in size.
Additionally, 82\% percent of entities contain less than 1000 mentions.
These number suggest that at times we we can take advantage of the redundancy within
large entities and compress the entities.
We investigate the Wiki links corpus further in Section~\ref{sec:optimizer:wikilinkcorpus}. 


In addition, Figure~\ref{fig:entity-distribution} shows that there is an
order of magnitude difference between the sizes of initial entities and the
true entity sizes.
This difference gives us some intuition of the trends of the entity resolution process.
The entities were initialized by exact string match, a common initialization scheme.
This suggest that there are several unique representations
of entities that during entity resolution entities sizes can expect to grow 
by an order of magnitude in size and the number of smaller entities will decrease.
This is to be expected but we can use this property to track the growth and
change of entity sizes over time to understand which camp a particular entity
cluster belongs. 



